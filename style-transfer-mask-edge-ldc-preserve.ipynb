{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7LwZCdRovKII2ly21m07j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')\n","\n","import cv2\n","import torch\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"Y5f9F2oK_CuA","executionInfo":{"status":"ok","timestamp":1671008856098,"user_tz":300,"elapsed":22199,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae78e366-c040-495a-b04c-6d0916c3b45a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/cv_project_fall_2022')\n","\n","import utils.utils as utils\n","from models.definitions.transformer_net_new import TransformerNet\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import transforms"],"metadata":{"id":"HBabeISmtEhw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671008898496,"user_tz":300,"elapsed":5050,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"356903ae-988e-4e2c-87c7-64baffdb6ce0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def stylize_static_image(inference_config):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Prepare the model - load the weights and put the model into evaluation mode\n","    stylization_model = TransformerNet().to(device)\n","    model_saved_path = inference_config['model_saved_path']\n","    #training_state = torch.load(os.path.join(inference_config[\"model_binaries_path\"], inference_config[\"model_name\"]))\n","    training_state = torch.load(model_saved_path)\n","    state_dict = training_state[\"state_dict\"]\n","    stylization_model.load_state_dict(state_dict, strict=True)\n","    stylization_model.eval()\n","\n","    if inference_config['verbose']:\n","        utils.print_model_metadata(training_state)\n","\n","    content_img_path = inference_config['content_images_path'] #os.path.join(inference_config['content_images_path'], inference_config['content_input'])\n","    print(content_img_path)\n","    content_image_to_show = Image.open(content_img_path)\n","    plt.imshow(content_image_to_show)\n","    plt.show()\n","\n","    style_image_path = inference_config['style_images_path']\n","    style_image_to_show = Image.open(style_image_path)\n","    plt.imshow(style_image_to_show)\n","    plt.show()\n","\n","    with torch.no_grad():\n","        if os.path.isdir(inference_config['content_input']):  # do a batch stylization (every image in the directory)\n","            img_dataset = utils.SimpleDataset(inference_config['content_input'], inference_config['img_width'])\n","            img_loader = DataLoader(img_dataset, batch_size=inference_config['batch_size'])\n","\n","            try:\n","                processed_imgs_cnt = 0\n","                for batch_id, img_batch in enumerate(img_loader):\n","                    processed_imgs_cnt += len(img_batch)\n","                    if inference_config['verbose']:\n","                        print(f'Processing batch {batch_id + 1} ({processed_imgs_cnt}/{len(img_dataset)} processed images).')\n","\n","                    img_batch = img_batch.to(device)\n","                    stylized_imgs = stylization_model(img_batch).to('cpu').numpy()\n","                    for stylized_img in stylized_imgs:\n","                        utils.save_and_maybe_display_image(inference_config, stylized_img, should_display=False)\n","            except Exception as e:\n","                print(e)\n","                print(f'Consider making the batch_size (current = {inference_config[\"batch_size\"]} images) or img_width (current = {inference_config[\"img_width\"]} px) smaller')\n","                exit(1)\n","\n","        else:  # do stylization for a single image\n","            #content_img_path = os.path.join(inference_config['content_images_path'], inference_config['content_input'])\n","            transform_list = [transforms.ToTensor()]\n","            transform = transforms.Compose(transform_list)\n","            \n","            target_transform_list = [transforms.ToTensor(), transforms.GaussianBlur(kernel_size=(51, 51), sigma=(70, 70))]\n","            #target_transform_list = [transforms.ToTensor(), transforms.GaussianBlur(kernel_size=(21, 21), sigma=(50, 50))]\n","            transform_target = transforms.Compose(target_transform_list)\n","            \n","            img = utils.load_image(content_img_path, target_shape=inference_config['img_width'])\n","            img = transform(img).to(device)\n","\n","            \n","            face_masked_image_batch = utils.load_image(inference_config['content_images_mask_path'], target_shape=inference_config['img_width'], RGB=False)\n","            face_masked_image_batch = transform_target(face_masked_image_batch).to(device)\n","            face_masked_image_batch = face_masked_image_batch * 0.03\n","\n","            result = torch.cat([img, face_masked_image_batch], dim=0)\n","\n","            print(' Infering...')\n","            print(img.shape)\n","            content_image = result.repeat(1, 1, 1, 1)\n","            print(content_image.shape)\n","            print(' Infering Ends...')\n","            #content_image = utils.prepare_img(content_img_path, inference_config['img_width'], device)\n","            stylized_img = stylization_model(content_image).to('cpu').numpy()[0]\n","            \n","            \n","            \n","            utils.save_and_maybe_display_image(inference_config, stylized_img, should_display=inference_config['should_not_display'])\n"],"metadata":{"id":"LYGOAz1ItWAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["content_images_path = os.path.join('/content/drive/MyDrive/cv_project_fall_2022', 'data', 'content-images')\n","output_images_path = os.path.join('/content/drive/MyDrive/cv_project_fall_2022', 'data', 'output-images')\n","model_binaries_path = os.path.join('/content/drive/MyDrive/cv_project_fall_2022', 'models', 'binaries')\n","\n","assert utils.dir_contains_only_models(model_binaries_path), f'Model directory should contain only model binaries.'\n","os.makedirs(output_images_path, exist_ok=True)\n","\n","#\n","# Modifiable args - feel free to play with these\n","#\n","# parser = argparse.ArgumentParser()\n","# # Put image name or directory containing images (if you'd like to do a batch stylization on all those images)\n","# parser.add_argument(\"--content_input\", type=str, help=\"Content image(s) to stylize\", default='taj_mahal.jpg')\n","# parser.add_argument(\"--batch_size\", type=int, help=\"Batch size used only if you set content_input to a directory\", default=5)\n","# parser.add_argument(\"--img_width\", type=int, help=\"Resize content image to this width\", default=500)\n","# parser.add_argument(\"--model_name\", type=str, help=\"Model binary to use for stylization\", default='mosaic_4e5_e2.pth')\n","\n","# # Less frequently used arguments\n","# parser.add_argument(\"--should_not_display\", action='store_false', help=\"Should display the stylized result\")\n","# parser.add_argument(\"--verbose\", action='store_true', help=\"Print model metadata (how the model was trained) and where the resulting stylized image was saved\")\n","# parser.add_argument(\"--redirected_output\", type=str, help=\"Overwrite default output dir. Useful when this project is used as a submodule\", default=None)\n","# args = parser.parse_args()\n","\n","# # if redirected output is not set when doing batch stylization set to default image output location\n","# if os.path.isdir(args.content_input) and args.redirected_output is None:\n","#     args.redirected_output = output_images_path\n","\n","# Wrapping inference configuration into a dictionary\n","def get_mask_path(file_path):\n","    file_name = file_path.split('/')[-1]\n","    file_name_split = file_name.split('_')\n","    dir_name = file_name_split[0] + '--' + file_name_split[1]\n","    return file_path.replace(\"/test_images/\", \"/data/widerface/WIDER_train/masks/\" + dir_name + \"/\")\n","\n","inference_config = dict()\n","\n","\n","inference_config['content_input'] = 'lion.jpg'\n","inference_config['batch_size'] = 5\n","inference_config['img_width'] = 500\n","\n","inference_config['model_name'] = 'mosaic_4e5_e2.pth'\n","inference_config['style_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/data/style-images/mosaic.jpg'\n","# 1st conv layer\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_content_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_9_batch_1899.pth'\n","# 3rd conv layer\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_content_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","# 1st conv layer\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_1st_layer_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","\n","# 2nd conv layer\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_1st_layer_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","\n","# 2nd conv layer - using content and style loss of layer 2\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/ckpt_style_face_layer_2_mosaic_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","#inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/mosaic/ckpt_style_face_layer_2_mosaic_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","\n","#inference_config['model_saved_path'] = \"/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_3rd_layer_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth\"\n","\n","\n","#inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/edtaonisl/ckpt_style_face_edtaonisl_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","\n","    \n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/mosaic/ckpt_style_depth_face_edge_layer_mosaic_cw_1.0_sw_400000.0_tw_0_epoch_3_batch_1899.pth'\n","inference_config['model_saved_path'] = '/content/drive/MyDrive/cv_project_fall_2022/models/checkpoints/mosaic/ckpt_style_face_edge_only_new_mosaic_cw_1.0_sw_400000.0_tw_0_epoch_6_batch_1899.pth'\n","\n","inference_config['should_not_display'] = True\n","inference_config['verbose'] = True\n","# inference_config['redirected_output'] = True\n","\n","# inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/data/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_641.jpg'\n","inference_config['output_images_path'] = output_images_path\n","inference_config['model_binaries_path'] = model_binaries_path\n","inference_config['redirected_output'] = output_images_path\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/0_Parade_Parade_0_467.jpg'\n","inference_config['content_images_mask_path'] = get_mask_path(inference_config['content_images_path'])\n","stylize_static_image(inference_config)\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/0_Parade_marchingband_1_1031.jpg'\n","inference_config['content_images_mask_path'] = get_mask_path(inference_config['content_images_path'])\n","stylize_static_image(inference_config)\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/0_Parade_Parade_0_904.jpg'\n","inference_config['content_images_mask_path'] = get_mask_path(inference_config['content_images_path'])\n","stylize_static_image(inference_config)\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/8_Election_Campain_Election_Campaign_8_36.jpg'\n","inference_config['content_images_mask_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/data/widerface/WIDER_train/masks/8--Election_Campain/8_Election_Campain_Election_Campaign_8_36.jpg' \n","stylize_static_image(inference_config)\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/8_Election_Campain_Election_Campaign_8_138.jpg'\n","inference_config['content_images_mask_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/data/widerface/WIDER_train/masks/8--Election_Campain/8_Election_Campain_Election_Campaign_8_138.jpg' \n","stylize_static_image(inference_config)\n","\n","inference_config['content_images_path'] = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/19_Couple_Couple_19_940.jpg'\n","inference_config['content_images_mask_path'] = get_mask_path(inference_config['content_images_path'])\n","stylize_static_image(inference_config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1EHP04zCgaIcuYgAy1DSrxmjG1OXApyaF"},"id":"h23_r4lCuOgJ","executionInfo":{"status":"ok","timestamp":1671009026340,"user_tz":300,"elapsed":8894,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"c8a230b5-56f5-4476-fa77-c41d84b65b59"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# import face_mask"],"metadata":{"id":"UvdVSTTRuk_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://github.com/onnx/models#body_analysis"],"metadata":{"id":"a45iNe9AdcJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/"],"metadata":{"id":"qbzFMAPFoPdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://modelzoo.co/"],"metadata":{"id":"VcN4b2n5oPfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://github.com/topics/face-segmentation"],"metadata":{"id":"MnPyixf9rd6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/0_Parade_Parade_0_467.jpg'\n","file_name = file_path.split('/')[-1]\n","file_name_split = file_name.split('_')\n","dir_name = file_name_split[0] + '--' + file_name_split[1]\n","file_path.replace(\"/test_images/\", \"/data/widerface/WIDER_train/masks/\" + dir_name + \"/\" + file_name)"],"metadata":{"id":"DAIvoIjuOapH","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1670664396180,"user_tz":300,"elapsed":25,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"b19d098d-8285-4fff-85d8-1ade89ae7db6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/cv_project_fall_2022/face_mask/data/widerface/WIDER_train/masks/0--Parade/0_Parade_Parade_0_467.jpg0_Parade_Parade_0_467.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"9D9m3Gtlci3g","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1671003094001,"user_tz":300,"elapsed":1085,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"f5d7aca9-6ee0-4b24-b348-774f0edb2b99"},"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-808fd3782168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/cv_project_fall_2022/face_mask/test_images/0_Parade_Parade_0_467.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Read image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yLbLtG38i7T6"},"execution_count":null,"outputs":[]}]}