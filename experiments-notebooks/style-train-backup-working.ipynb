{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGlGyw12qA2cg3a0PRU7Ew"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eS4AXxyKldRG","executionInfo":{"status":"ok","timestamp":1669879305324,"user_tz":300,"elapsed":22894,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"197fca36-8e65-4c83-d873-b516c5652521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l53NZwuQg2eB"},"outputs":[],"source":["#!python /content/drive/MyDrive/cv_project_fall_2022/utils/resource_downloader.py -r mscoco_dataset"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/cv_project_fall_2022')"],"metadata":{"id":"3OeO-LsA8M_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"6RS5DtbTKE2G"}},{"cell_type":"code","source":["import os\n","import argparse\n","import time\n","\n","import torch\n","from torch.optim import Adam\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","\n","from models.definitions.perceptual_loss_net import PerceptualLossNet\n","from models.definitions.transformer_net import TransformerNet\n","import utils.utils as utils"],"metadata":{"id":"QlfMR8yDNUc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def train(training_config):\n","    writer = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # prepare data loader\n","    train_loader = utils.get_training_data_loader(training_config)\n","\n","    # prepare neural networks\n","    transformer_net = TransformerNet().train().to(device)\n","    perceptual_loss_net = PerceptualLossNet(requires_grad=False).to(device)\n","\n","    optimizer = Adam(transformer_net.parameters())\n","\n","    # Calculate style image's Gram matrices (style representation)\n","    # Built over feature maps as produced by the perceptual net - VGG16\n","    style_img_path = os.path.join(training_config['style_images_path'], training_config['style_img_name'])\n","    style_img = utils.prepare_img(style_img_path, target_shape=None, device=device, batch_size=training_config['batch_size'])\n","    style_img_set_of_feature_maps = perceptual_loss_net(style_img)\n","    target_style_representation = [utils.gram_matrix(x) for x in style_img_set_of_feature_maps]\n","\n","    utils.print_header(training_config)\n","    # Tracking loss metrics, NST is ill-posed we can only track loss and visual appearance of the stylized images\n","    acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n","    ts = time.time()\n","    for epoch in range(training_config['num_of_epochs']):\n","        for batch_id, (content_batch, _) in enumerate(train_loader):\n","            # step1: Feed content batch through transformer net\n","            print(\"Content Batch : \")\n","            print(type(content_batch))\n","            print(content_batch.shape)\n","            #print(type(content_batch[0]))\n","            print(type(_))\n","            print(_.shape)\n","            print(_)\n","            break\n","            content_batch = content_batch.to(device)\n","            stylized_batch = transformer_net(content_batch)\n","\n","            # step2: Feed content and stylized batch through perceptual net (VGG16)\n","            content_batch_set_of_feature_maps = perceptual_loss_net(content_batch)\n","            stylized_batch_set_of_feature_maps = perceptual_loss_net(stylized_batch)\n","\n","            # step3: Calculate content representations and content loss\n","            target_content_representation = content_batch_set_of_feature_maps.relu2_2\n","            current_content_representation = stylized_batch_set_of_feature_maps.relu2_2\n","            content_loss = training_config['content_weight'] * torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n","\n","            # step4: Calculate style representation and style loss\n","            style_loss = 0.0\n","            current_style_representation = [utils.gram_matrix(x) for x in stylized_batch_set_of_feature_maps]\n","            for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n","                style_loss += torch.nn.MSELoss(reduction='mean')(gram_gt, gram_hat)\n","            style_loss /= len(target_style_representation)\n","            style_loss *= training_config['style_weight']\n","\n","            # step5: Calculate total variation loss - enforces image smoothness\n","            tv_loss = training_config['tv_weight'] * utils.total_variation(stylized_batch)\n","\n","            # step6: Combine losses and do a backprop\n","            total_loss = content_loss + style_loss + tv_loss\n","            total_loss.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()  # clear gradients for the next round\n","\n","            #\n","            # Logging and checkpoint creation\n","            #\n","            acc_content_loss += content_loss.item()\n","            acc_style_loss += style_loss.item()\n","            acc_tv_loss += tv_loss.item()\n","\n","            if training_config['enable_tensorboard']:\n","                # log scalars\n","                writer.add_scalar('Loss/content-loss', content_loss.item(), len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalar('Loss/style-loss', style_loss.item(), len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalar('Loss/tv-loss', tv_loss.item(), len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalars('Statistics/min-max-mean-median', {'min': torch.min(stylized_batch), 'max': torch.max(stylized_batch), 'mean': torch.mean(stylized_batch), 'median': torch.median(stylized_batch)}, len(train_loader) * epoch + batch_id + 1)\n","                # log stylized image\n","                if batch_id % training_config['image_log_freq'] == 0:\n","                    stylized = utils.post_process_image(stylized_batch[0].detach().to('cpu').numpy())\n","                    stylized = np.moveaxis(stylized, 2, 0)  # writer expects channel first image\n","                    writer.add_image('stylized_img', stylized, len(train_loader) * epoch + batch_id + 1)\n","\n","            if training_config['console_log_freq'] is not None and batch_id % training_config['console_log_freq'] == 0:\n","                print(f'time elapsed={(time.time()-ts)/60:.2f}[min]|epoch={epoch + 1}|batch=[{batch_id + 1}/{len(train_loader)}]|c-loss={acc_content_loss / training_config[\"console_log_freq\"]}|s-loss={acc_style_loss / training_config[\"console_log_freq\"]}|tv-loss={acc_tv_loss / training_config[\"console_log_freq\"]}|total loss={(acc_content_loss + acc_style_loss + acc_tv_loss) / training_config[\"console_log_freq\"]}')\n","                acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n","\n","            if training_config['checkpoint_freq'] is not None and (batch_id + 1) % training_config['checkpoint_freq'] == 0:\n","                training_state = utils.get_training_metadata(training_config)\n","                training_state[\"state_dict\"] = transformer_net.state_dict()\n","                training_state[\"optimizer_state\"] = optimizer.state_dict()\n","                ckpt_model_name = f\"ckpt_style_{training_config['style_img_name'].split('.')[0]}_cw_{str(training_config['content_weight'])}_sw_{str(training_config['style_weight'])}_tw_{str(training_config['tv_weight'])}_epoch_{epoch}_batch_{batch_id}.pth\"\n","                torch.save(training_state, os.path.join(training_config['checkpoints_path'], ckpt_model_name))\n","\n","    #\n","    # Save model with additional metadata - like which commit was used to train the model, style/content weights, etc.\n","    #\n","    training_state = utils.get_training_metadata(training_config)\n","    training_state[\"state_dict\"] = transformer_net.state_dict()\n","    training_state[\"optimizer_state\"] = optimizer.state_dict()\n","    model_name = f\"style_{training_config['style_img_name'].split('.')[0]}_datapoints_{training_state['num_of_datapoints']}_cw_{str(training_config['content_weight'])}_sw_{str(training_config['style_weight'])}_tw_{str(training_config['tv_weight'])}.pth\"\n","    torch.save(training_state, os.path.join(training_config['model_binaries_path'], model_name))\n"],"metadata":{"id":"QQcIKzEaffAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Fixed args - don't change these unless you have a good reason\n","#\n","root = \"/content/drive/MyDrive/cv_project_fall_2022\"\n","dataset_path = os.path.join(root, 'data', 'mscoco') #,'train2017')\n","style_images_path = os.path.join(root, 'data', 'style-images')\n","model_binaries_path = os.path.join(root, 'models', 'binaries')\n","checkpoints_root_path = os.path.join(root, 'models', 'checkpoints')\n","image_size = 256  # training images from MS COCO are resized to image_size x image_size\n","batch_size = 4\n","\n","assert os.path.exists(dataset_path), f'MS COCO missing. Download the dataset using resource_downloader.py script.'\n","os.makedirs(model_binaries_path, exist_ok=True)\n"],"metadata":{"id":"3TZxLuE_ffDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#\n","# Modifiable args - feel free to play with these (only a small subset is exposed by design to avoid cluttering)\n","#\n","default_config = dict()\n","parser = argparse.ArgumentParser()\n","# training related\n","#parser.add_argument(\"--style_img_name\", type=str, help=\"style image name that will be used for training\", default='edtaonisl.jpg')\n","default_config['style_img_name'] = 'edtaonisl.jpg'\n","\n","#parser.add_argument(\"--content_weight\", type=float, help=\"weight factor for content loss\", default=1e0)  # you don't need to change this one just play with style loss\n","default_config['content_weight'] = 1e0\n","#parser.add_argument(\"--style_weight\", type=float, help=\"weight factor for style loss\", default=4e5)\n","default_config['style_weight'] = 4e5\n","#parser.add_argument(\"--tv_weight\", type=float, help=\"weight factor for total variation loss\", default=0)\n","default_config['tv_weight'] = 0\n","#parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs \", default=2)\n","default_config['num_of_epochs'] = 2\n","#parser.add_argument(\"--subset_size\", type=int, help=\"number of MS COCO images (NOT BATCHES) to use, default is all (~83k)(specified by None)\", default=None)\n","default_config['subset_size'] = 100\n","# logging/debugging/checkpoint related (helps a lot with experimentation)\n","#parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging (scalars + images)\", default=True)\n","default_config['enable_tensorboard'] = False\n","#parser.add_argument(\"--image_log_freq\", type=int, help=\"tensorboard image logging (batch) frequency - enable_tensorboard must be True to use\", default=100)\n","#default_config['image_log_freq'] = 100\n","#parser.add_argument(\"--console_log_freq\", type=int, help=\"logging to output console (batch) frequency\", default=500)\n","default_config['console_log_freq'] = 10\n","#parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (batch) frequency\", default=2000)\n","default_config['checkpoint_freq'] = 100\n","#args = parser.parse_args()\n","\n"],"metadata":{"id":"Bf5LWFnD87EZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVMr2YVP8liU","executionInfo":{"status":"ok","timestamp":1669879583065,"user_tz":300,"elapsed":2,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"f0a60d34-fb08-44e9-82e4-6dda831d9a9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/cv_project_fall_2022/data/mscoco\n"]}]},{"cell_type":"code","source":["checkpoints_path = os.path.join(checkpoints_root_path,default_config['style_img_name'].split('.')[0])\n","if default_config['checkpoint_freq'] is not None:\n","    os.makedirs(checkpoints_path, exist_ok=True)\n","\n","# Wrapping training configuration into a dictionary\n","training_config = dict()\n","for arg in default_config.keys():\n","    training_config[arg] = default_config[arg]\n","training_config['dataset_path'] = dataset_path\n","training_config['style_images_path'] = style_images_path\n","training_config['model_binaries_path'] = model_binaries_path\n","training_config['checkpoints_path'] = checkpoints_path\n","training_config['image_size'] = image_size\n","training_config['batch_size'] = batch_size\n","\n","# Original J.Johnson's training with improved transformer net architecture\n","train(training_config)\n","\n"],"metadata":{"id":"WRvrHQrFffF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669879585807,"user_tz":300,"elapsed":1412,"user":{"displayName":"Rahul Meghwal","userId":"07649752308203231636"}},"outputId":"63dd82e1-bc7e-4999-d634-203dd4063441"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using 200 datapoints (50 batches) (MS COCO images) for transformer network training.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Learning the style of edtaonisl.jpg style image.\n","********************************************************************************\n","Hyperparams: content_weight=1.0, style_weight=400000.0 and tv_weight=0\n","********************************************************************************\n","Logging to console every 10 batches.\n","Saving checkpoint models every 100 batches.\n","Tensorboard disabled.\n","********************************************************************************\n","Content Batch : \n","<class 'torch.Tensor'>\n","torch.Size([4, 3, 256, 256])\n","<class 'torch.Tensor'>\n","torch.Size([4])\n","tensor([0, 0, 0, 0])\n","Content Batch : \n","<class 'torch.Tensor'>\n","torch.Size([4, 3, 256, 256])\n","<class 'torch.Tensor'>\n","torch.Size([4])\n","tensor([0, 0, 0, 0])\n"]}]},{"cell_type":"code","source":["# Using 200 datapoints (50 batches) (MS COCO images) for transformer network training.\n","# /usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","#   warnings.warn(\n","# /usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","#   warnings.warn(msg)\n","# Learning the style of edtaonisl.jpg style image.\n","# ********************************************************************************\n","# Hyperparams: content_weight=1.0, style_weight=400000.0 and tv_weight=0\n","# ********************************************************************************\n","# Logging to console every 10 batches.\n","# Saving checkpoint models every 100 batches.\n","# Tensorboard disabled.\n","# ********************************************************************************\n","# Content Batch : \n","# <class 'torch.Tensor'>\n","# torch.Size([4, 3, 256, 256])\n","# <class 'torch.Tensor'>\n","# torch.Size([4])\n","# tensor([0, 0, 0, 0])\n","# Content Batch : \n","# <class 'torch.Tensor'>\n","# torch.Size([4, 3, 256, 256])\n","# <class 'torch.Tensor'>\n","# torch.Size([4])\n","# tensor([0, 0, 0, 0])"],"metadata":{"id":"c7pblWz5ffIi"},"execution_count":null,"outputs":[]}]}